---
layout: post
title: 클러스터링 분석 - (3) 스케일 조정
summary: 클러스터링 분석, 세번째 시간. 스케일 조정(scaling) 방법에 대하여 알아봅니다.
featured-img: iris3
categories: 클러스터링분석 스케일링 전처리
mathjax: true
---

오늘은 데이터 **스케일 조정(scaling)**에 대한 이야기입니다.

<br>
#### *- 클러스터링 분석, 다른 이야기들 -*

[1]:https://hweejin-lim.github.io/%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%EB%A7%81-%EB%B6%84%EC%84%9D-(1)-%EB%B2%94%EC%A3%BC%ED%98%95-%EB%B3%80%EC%88%98/
[2]:https://hweejin-lim.github.io/%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%EB%A7%81-%EB%B6%84%EC%84%9D-(2)-%EA%B2%B0%EC%B8%A1%EA%B0%92/
[3]:https://hweejin-lim.github.io/%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%EB%A7%81-%EB%B6%84%EC%84%9D-(3)-%EC%8A%A4%EC%BC%80%EC%9D%BC-%EC%A1%B0%EC%A0%95/
[클러스터링 분석 - (1) 범주형 변수][1]  
[클러스터링 분석 - (2) 결측값][2]  
[클러스터링 분석 - (3) 스케일 조정][3]  

<br>

### 데이터 스케일이란?

데이터 **스케일(scale)**은 데이터의 **크기, 규모, 범위**를 의미합니다. 

|이름|간식비용(원)|식사비용(원)|
|:-:|:-:|:-:|
|종원|10,000|75,000|
|여운|0|25,000|
|상인|6,000|60,000|
|지윤|8,000|55,000|

예시를 한 번 보실까요? 4명의 친구가 지난 일주일간 간식 구입과 점심 식대로 지불한 비용을 표로 정리했습니다. 간식 비용은 0~10,000원에서 값이 분포되어 있고, 식사비용은 25,000~75,000원 사이에 값이 분포되어 있습니다.  

변수끼리 스케일 차이가 크게 벌어지게 되면, 값을 비교할 때 판단을 쉽게 할 수 없는 문제가 발생합니다. 예를 들어, 식사 비용보다 간식 비용에 상대적으로 돈을 더 많이 지불한 사람은 누구일까요? 머릿속으로 쉽게 계산하기 어렵습니다. 만약 비교해야 할 사람도 많고 변수의 종류도 많다면 비교가 더욱 어려워지겠죠?

데이터 스케일링은 **개체나 변수간의 비교 혹은 차이 계산이 필요한 상황**에서 **스케일 조정**을 통해 그런 작업들을 보다 수월하게 해주는 것을 의미합니다. 다음과 같이 스케일 조정을 거친 변수를 하나 더 추가해 볼께요.

|이름|간식비용(원)|식사비용(원)|식사비용(원)_scaled|
|:-:|:-:|:-:|:-:|
|종원|10,000|75,000|10,000|
|여운|0|25,000|0|
|상인|6,000|60,000|7,000|
|지윤|8,000|55,000|6,000|

스케일을 조정한 변수는 가장 많은 식사 비용이었던 75,000을 가장 높은 간식 비용인 10,000원에 맞추고, 가장 적은 식사 비용인 25,000을 가장 적은 간식 비용이었던 0에 맞추어서 비율에 맞게 다시 계산한 값으로 채웠습니다. 이번에는 아까 했던 질문에 좀더 쉽게 답을 할 수 있을 것 같습니다. 누가 식사 비용보다 간식 비용에 좀더 돈을 지불한 셈일까요? *(정답은 지윤 ! )*




<br>
### 클러스터링 분석을 위한 필수 작업

이전에 범주형 변수와 결측값 이야기를 다루면서 **거리 기반 클러스터링 분석에서는 개체끼리 특성 차이가 얼마나 나는지 계산이 필요하다**는 이야기를 했던 것 기억하시나요? 클러스터링 분석에서는 단일 변수(특성)에 대한 차이 계산뿐만 아니라, 둘 이상의 변수들을 모두 고려하여 개체간의 차이 계산이 필요합니다. (왜냐하면 어떤 개체끼리 비슷한지 판단하기 위해 보통 여러가지의 특성들을 고려하기 때문입니다.) 그렇기 때문에 **변수끼리 스케일 차이가 크게 벌어지면 스케일이 큰 변수의 영향력이 상대적으로 커지기 때문에 분석 결과가 잘못될 가능성이 높아지게 됩니다.**

**스케일 조정이 클러스터링 분석에 앞서 반드시 필요한 이유**도 바로 여기에 있습니다. 거리 기반 알고리즘을 사용하는 클러스터링 분석 뿐만 아니라 신경망 모형, SVM 같은 알고리즘도 데이터 스케일에 민감하기 때문에 데이터의 특성값을 조정하는 방법에 대해 미리 알아두는 것도 필요합니다. 


<br>
### 여러가지 스케일 조정 방법

보통은 각각의 변수별로 전처리 작업을 진행한 후 분석 혹은 모델링 작업에 사용할 변수 모두를 일괄적으로 스케일 조정하는 과정을 거치게 됩니다. 아래는 대표적인 스케일링 방법들이고, 프로그램 언어 라이브러리마다 쉽게 활용할 수 있는 모듈을 제공하고 있습니다.

#### 1. Standard Scaler
```
z = (X - u) / s
※ u : 변수 X의 평균, s : 변수 X의 표준편차
```
 [[참고] sklearn.preprocessing.StandardScaler]('https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler')
- 각 변수(특성)의 평균을 0, 분산을 1로 변경하여 모든 변수가 같은 크기를 갖게 함.
- 변수의 최솟값과 최댓값 크기를 제한하지 않음. 
- 따라서, 이상값(outlier)이 있는 경우 평균과 표준편차에 영향을 미치기 때문에 데이터 분포(확산)에 영향을 미침.

#### 2. MinMax Scaler
```
X_std = (X - X.min) / (X.max - X.min)
X_scaled = X_std * (max - min) + min
```
[[참고] sklearn.preprocessing.MinMaxScaler]('https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler')
- 모든 특성이 정확하게 0과 1 사이에 위치하도록 데이터를 변형
- Standard scaler와 마찬가지로 이상값의 영향에 민감함.

#### 3. Robust Scaler
```
X_scaled = (X - q2) / (q3 - q1)
※ q2 : 중앙값, q1 : 1사분위값, q3 : 3사분위값
```
[[참고] sklearn.preprocessing.RobustScaler]('https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler')
- 위의 2가지 방법과는 다르게 이상값의 영향을 최소화
- 중앙값(medain)과 사분위값(quartile)을 사용

위의 3가지 이외에도 [Normalizer]('https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html#sklearn.preprocessing.normalize'), [QuantileTransformer]('https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html#sklearn.preprocessing.QuantileTransformer') 등의 스케일링 방법도 있습니다. 어떤 스케일 조정 방법을 사용할지는 데이터의 특징, 분포 등을 검토한 후에 결정하면 됩니다. 

저의 경우에는 최근 회사에서 클러스터링 분석을 진행할 때 변수별로 이상값(그 중에서도 특히 과대값) 전처리를 진행한 후 MinMaxScaler를 적용했습니다. 사실 RobustScaler와 MinMaxScaler를 모두 적용하여 클러스터링 결과를 비교해보았는데요, MinMaxScaler 방식을 적용한 결과가 클러스터간 특징 구분이 좀더 수월한 면이 있었기 때문에 최종적으로는 MinMaxScaler를 사용했습니다. 꼭 한 가지 방법만을 고집할 필요는 없을 것 같고요, 클러스터링 분석 자체가 워낙 여러가지 다양한 시도를 반복해야하는 작업인지라 이런저런 방법을 시도해 보시고 좀더 적절한 방법을 선택하시면 될 것 같습니다. 


<br>
### 다음 시간에는..

3번에 걸쳐 클러스터링 분석용 데이터 준비 작업에 대한 이야기를 했는데요. 다음에는 **데이터 전처리를 할 때 자주 사용하는 Python 코드**에 대해서 한 번 정리해볼까 합니다. 오늘도 긴 글 읽어주셔서 감사합니다!!














