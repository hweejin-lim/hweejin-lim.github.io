---
layout: post
title: 클러스터링 분석 - (6) 결과 시각화
summary: 다차원 데이터의 클러스터링 결과를 좀더 효과적으로 시각화하는 방법을 생각해 봅시다.
featured-img: iris6
categories: 클러스터링분석 시각화
mathjax: true
---

다차원 데이터, 즉 많은 변수를 사용한 클러스터링 결과를 좀더 빠르게 확인하기 위해서는 어떻게 해야할까요? 

대부분의 클러스터링 분석 예제를 보면 둘 내지 세개의 변수를 축으로 산포도(scatter plot) 혹은 3D plot을 그려 클러스터가 어떤 기준으로 묶였는지 확인하는 방법을 제시하고 있습니다. 하지만, 실제 분석 상황에서는 3개 이상의 변수를 활용한 경우가 대부분이고, 클러스터의 수 역시 많아질 수 있기 때문에 산포도를 그려 군집이 어떤 특성을 갖고 있는지 확인하는 방법에는 한계가 존재합니다. 오늘은 기본적인 시각화 방법을 활용하여 클러스터링 결과를 빠르게 확인하는 방법에 대한 이야기를 해보고자 합니다.


<br>
#### *- 클러스터링 분석, 다른 이야기들 -*

아래는 클러스터링 분석과 관련하여 다루었던 다른 이야기들입니다. 참고로 보시면 좋을 것 같아요.

[1]:https://hweejin-lim.github.io/%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%EB%A7%81-%EB%B6%84%EC%84%9D-(1)-%EB%B2%94%EC%A3%BC%ED%98%95-%EB%B3%80%EC%88%98/
[2]:https://hweejin-lim.github.io/%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%EB%A7%81-%EB%B6%84%EC%84%9D-(2)-%EA%B2%B0%EC%B8%A1%EA%B0%92/
[3]:https://hweejin-lim.github.io/%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%EB%A7%81-%EB%B6%84%EC%84%9D-(3)-%EC%8A%A4%EC%BC%80%EC%9D%BC-%EC%A1%B0%EC%A0%95/
[4]:https://hweejin-lim.github.io/%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%EB%A7%81-%EB%B6%84%EC%84%9D-(4)-%EB%B3%80%EB%B3%84%EB%A0%A5-%EC%9E%88%EB%8A%94-%EB%B3%80%EC%88%98-%EC%B0%BE%EA%B8%B0/
[5]:https://hweejin-lim.github.io/%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%EB%A7%81-%EB%B6%84%EC%84%9D-(5)-%EC%A0%81%EC%A0%95-%EA%B5%B0%EC%A7%91%EC%88%98(k)-%EC%B0%BE%EA%B8%B0/
[6]:http:
[클러스터링 분석 - (1) 범주형 변수][1]    
[클러스터링 분석 - (2) 결측값][2]    
[클러스터링 분석 - (3) 스케일 조정][3]    
[클러스터링 분석 - (4) 변별력 있는 변수 찾기][4]    
[클러스터링 분석 - (5) 적정 군집수(k) 찾기][5]     
[클러스터링 분석 - (6) 결과 시각화][6]

<br>




### 1단계 : 클러스터의 크기 확인

우선 가장 먼저 확인할 부분은 **각 클러스터의 크기가 어떠한지, 고르게 나뉘었는지 확인**하는 것입니다.    

만약 클러스터 종류가 많고, 크기가 작은 클러스터가 눈에 띈다면, 결과 검토 후 다른 클러스터와 합치는(같은 클러스터로 취급) 것을 고려해 볼 수 있습니다.    
    
```python
# df : 데이터셋
# cluster : 데이터셋에서 클러스터 결과 컬럼명

cluster_size = df['cluster'].value_count()
(혹은)
cluster_size = df.groupby(['cluster'])['id'].count()    # 'id' 대신 아무 컬럼 이름을 적어도 무방
print(cluster_size)
cluster_size.plot(kind = 'bar')
```
![cluster_size](https://drive.google.com/uc?id=1jKIaXa9rO2QeF48Wcbn1n-U5gOIMIzD0)





<br>
### 2단계 : 클러스터간 전반적인 특성 차이 한 눈에 보기

클러스터 크기를 확인했으면 그 다음은 **각 클러스터가 어떤 특징을 갖고 있는지 확인**할 차례입니다. 일단 '새의 눈'으로 전반적인 특징을 살펴보는 것이 좋을 것 같네요. 가장 널리 사용되고 유용한 기술통계량인 '평균'을 사용해보겠습니다. (데이터 특성에 따라 중위수나 최빈값을 사용해 보는 것도 좋습니다.) 클러스터 x 변수별 평균값 히트맵은 아래와 같이 그려볼 수 있습니다.    
    
```python
# df : 데이터셋
# cluster : 데이터셋에서 클러스터 결과 컬럼명

# 시각화에 필요한 모듈 불러오기



# 평균 데이터 테이블 만들기
temp = df.groupby(['cluster']).mean()    # 클러스터,변수별 평균 구하기
cluster_mean = temp.transpose()    # x축 ↔ y축 전환 (변수들이 y 축으로 배치되게끔)
mean_table = cluster_mean.div(cluster_mean.max(axis=1), axis=0)    # 모든 변수의 최대값이 1, 최소값이 0이 되도록 데이터 스케일 조정
# 위와 같이 데이터 스케일을 조정하는 이유는 히트맵을 그리면 값이 상대적으로 작은 변수의 클러스터간 특징 차이가 보이지 않기 때문이에요. 결국 눈에 잘 띄게 하기 위함!

# 히트맵으로 그리기
plt.figure(figsize = (20, 25))    # 히트맵 사이즈 설정
annot_kws= {'fontsize':12}    # 히트맵 폰트 사이즈 설정
sns.heatmap(mean_table,     # 히트맵 그릴 데이터셋
			annot=True,     # 레이블 표시 여부
            fmt='.3f',     # 레이블 표시 형식 (소숫점 3째자리까지)
            linewidths = 0.1,     # 히트맵 선 두께
            annot_kws = annot_kws,     # 아까 설정한 폰트 사이즈 적용
            cmap = 'RdYlBu_r')    # 컬러맵 설정
plt.title('클러스터 X 변수 mean table', fontsize=13)
plt.show()
```
![mean_table](https://drive.google.com/uc?id=1fLmJM5M4Jw382XA_TP8uD3xz1C85z08K)    

결과 테이블이 뭔가 알록달록 예쁜 것 같아서 이미지를 자르지 않고 통으로 한 번 붙여보았습니다 :)   

이런 형태라면 클러스터(x축)별로 어떤 변수값을 크게 갖고 있는지 혹은 그렇지 않은지 어느 정도 한눈에 확인이 가능할 것 같습니다. 또 모든 변수의 최대값과 최소값 스케일을 맞추면서 오히려 클러스터뿐만 아니라 변수간의 상대적인 크기 비교도 좀더 수월해지는 효과도 있는 것 같네요. 




<br>
### 3단계 : 변수마다 클러스터 차이 더 자세히 살펴보기

어느 정도 클러스터 윤곽이 그려졌다면 변수별로 클러스터간의 차이를 좀더 자세히 살펴볼 차례입니다.   

이 부분 역시 박스플롯(box plot)과 바그래프(bar plot)로 간단히 구현해볼 수 있습니다. 어떤 그래프를 사용할 지는 변수의 특징에 달려 있습니다. 0 혹은 1로 코딩된 변수(범주형변수를 dummy로 만든 변수나 변수가 어떤 행동이나 특징을 가지고 있는지 없는지 의미하는 변수)는 바그래프(평균)를 사용하면 되고, 일반적인 연속형 변수의 경우, 값의 분포까지 확인하기 위해 박스플롯을 그리는 것을 추천드립니다.    

```python
figure, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(nrows=2, ncols=3)   # 그래프를 2행 3열로 배치
figure.set_size_inches(30, 10)   # 그래프 전체 사이즈 설정

sns.barplot(data = df, y = 'cluster', x = '컬럼1', orient = 'h', ax = ax1)   # x축:값, y축:클러스터
sns.barplot(data = df, y = 'cluster', x = '컬럼2', orient = 'h', ax = ax2)
sns.boxplot(data = df, y = 'cluster', x = '컬럼3', orient = 'h', ax = ax3)   # boxplot
sns.barplot(data = df, y = 'cluster', x = '컬럼4', orient = 'h', ax = ax4)
sns.barplot(data = df, y = 'cluster', x = '컬럼5', orient = 'h', ax = ax5)
sns.barplot(data = df, y = 'cluster', x = '컬럼6', orient = 'h', ax = ax6)
```
![visual1](https://drive.google.com/uc?id=1JgCF7iStQgopyUvpO360N4iJkCnL9MQ-)    

위의 코드를 실행하면 나오는 결과입니다. 성격이 유사한 변수끼리 모아서 위와 같이 바그래프나 박스플롯을 적절히 섞어서 사용하면


<br>






<br>

### 군집 응집도, 비지도 학습의 한계

앞서 k-means 알고리즘을 사용할 때 활용가능한 2가지 군집 응집도 계산 방법을 살펴보았는데요, 그 어느 것도 적정 군집수를 마법처럼 콕 집어 알려주지는 않습니다. 다만, **아주 잘못된 결과를 처음부터 만드는 우를 범하지 않도록 시행착오를 줄여주는 데에 유용**하게 사용할 수 있을 것으로 보여집니다. 또한, 앞서 말씀드렸던 것처럼 비지도 학습은 정답이 없기 때문에 군집화 결과물을 평가하기 위해서는 **결국 사람이 직접 확인하는 과정**을 거칠 수밖에 없기도 합니다.     

실제로 저희 팀에서 최근에 진행한 고객세분화 프로젝트에서는 데이터 준비 작업과 군집화보다 **1)군집화 결과물을 이해하기 쉽게 시각화**하고, **2)현업(마케터, PO 등) 담당자들과 군집화가 잘 되었는지 검토**하는 과정에 훨씬 많은 시간을 쏟았던 것으로 기억합니다. 물론 이 과정에서 **군집의 수와 데이터 추출 조건을 달리하여 수차례 군집화 작업을 다시 진행**한 것은 당연한 일이겠지요. 프로파일링(군집별로 특성을 요약&시각화) 작업만 10번 가까이 반복했던 것으로 기억합니다. 그 과정에서 일부 클러스터(군집)는 활용 목적에 맞게 임의로 통합하는 작업을 병행하기도 했습니다. 

결국 클러스터링 분석 작업에서 납득할 만한, 그리고 현업에서 활용 가능한 결과물을 만들어내기 위해서는 온전히 기계에만 의존하는 것은 한계가 있습니다. 이것은 비단 k-means를 활용하는 작업뿐만 아니라 다른 알고리즘을 사용한 군집화 작업에서도 마찬가지입니다. 이렇게 보면 클러스터링 분석 결과물의 퀄리티는 '공들인 시간'과 상관관계가 높을 수도 있을 것 같네요.    

오늘 이야기는 여기서 마치겠습니다. 긴 글 읽어주셔서 감사합니다 :)    

<br>






















