---
layout: post
title: 클러스터링 분석 - (4) 적정 군집수(k) 찾기
summary: k-평균 군집화(k-means clustering)을 적용할 때 적정 군집수(k)는 어떻게 찾는 것이 좋을까요?
featured-img: iris4
categories: 클러스터링분석 k-means inertia
mathjax: true
---

대표적인 클러스터링 방법, **k-평균(k-means) 군집화** 알고리즘을 사용할 때 고민이 필요한 부분 중 한 가지는 **군집수(k)** 결정입니다. 군집분석은 **비지도 학습(unsupervised learning)** 방법 중 하나이고, 비지도 학습에서는 보통 타겟값 혹은 목표값이 없는 데이터를 사용하기 때문에 군집화가 잘 되었는지, 혹은 적정 클러스터(군집)의 수는 몇 개인지 판단하는 것은 매우 어렵습니다. 하지만, 분석가의 판단을 돕고, 최선의 결과를 도출하기 위한 방법으로 몇 가지를 생각해 볼 수 있는데요, 하나씩 살펴보도록 하겠습니다.


<br>
#### *- 클러스터링 분석, 다른 이야기들 -*

아래는 클러스터링 분석과 관련하여 다루었던 다른 이야기들입니다. 참고로 보시면 좋을 것 같아요.

[1]:https://hweejin-lim.github.io/%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%EB%A7%81-%EB%B6%84%EC%84%9D-(1)-%EB%B2%94%EC%A3%BC%ED%98%95-%EB%B3%80%EC%88%98/
[2]:https://hweejin-lim.github.io/%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%EB%A7%81-%EB%B6%84%EC%84%9D-(2)-%EA%B2%B0%EC%B8%A1%EA%B0%92/
[3]:https://hweejin-lim.github.io/%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%EB%A7%81-%EB%B6%84%EC%84%9D-(3)-%EC%8A%A4%EC%BC%80%EC%9D%BC-%EC%A1%B0%EC%A0%95/
[4]:https://hweejin-lim.github.io/%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%EB%A7%81-%EB%B6%84%EC%84%9D-(4)-%EC%A0%81%EC%A0%95-%EA%B5%B0%EC%A7%91%EC%88%98(k)-%EC%B0%BE%EA%B8%B0/
[클러스터링 분석 - (1) 범주형 변수][1]  
[클러스터링 분석 - (2) 결측값][2]  
[클러스터링 분석 - (3) 스케일 조정][3]  
[클러스터링 분석 - (4) 적정 군집수(k) 찾기][4]    

<br>



### Inertia value를 활용한 군집 응집도 탐색

Inertia value에 대해 설명하기 전에 **k-means 알고리즘의 동작 원리**를 간단히 알아보는 것이 좋을 것 같습니다.     

1. k개의 클러스터 중심(centroid)을 무작위로 지정
2. 데이터 포인트를 가장 가까운 클러스터 중심에 할당
3. 각각의 클러스터에 할당된 데이터 포인트의 평균으로 클러스터 중심을 다시 지정
4. 위의 2~3의 반복
5. 각각의 클러스터에 할당되는 데이터 포인트에 변화가 없을 때 알고리즘 종료

k-means 알고리즘을 통해 만들어진 각각의 군집은 하나의 중심(centroid)을 가지고 있고, 각 개체는 가장 가까운 중심에 할당됩니다. 이렇게 할당된 개체들이 모여 하나의 군집을 형성하는 방식으로 군집화가 진행됩니다. **Inertia value**는 **각 클러스터의 중심에서 클러스터에 할당된 데이터 포인트간의 거리를 합산**한 것을 의미하며, **군집이 얼마나 잘 응집**되었는지 보여주는 지표입니다. 이 값이 **작을수록 응집도가 높게 군집화가 잘 되었다**고 평가할 수 있습니다.   

따라서 데이터 전처리와 스케일링을 완료한 데이터셋을 이용하여 군집수(k)를 다양하게 적용하여 모델 학습 과정을 거친 후, Inertia value를 출력하여 군집수의 변화에 따라 응집도가 어떻게 달라지는지 확인할 수 있습니다. 아래는 Jupyter Notebook에서 Python으로 작성한 code이고, 실제로 최근에 분석에 활용했던 내용입니다. 굉장히 많은 변수와 개체로 구성된 dataset이었기 때문에 k의 수는 넉넉히 19개까지 설정하여 응집도 변화를 살펴보고자 했습니다.   


```python

# Import Module
from sklearn.cluster import KMeans    # k-means 모듈 불러오기
import matplotlib.pyplot as plt    # 시각화를 위한 matplotlib.pyplot 모듈 불러오기
%matplotlib inline    # 시각화 결과를 Jupyter Notebook에 바로 표시하기 위한 명령어

# k-means clustering & inertia simulation
ks = range(1,20)   # 1~19개의 k로 클러스터링하기 위함
inertias = []    # 응집도 결과 저장을 위한 빈 리스트 만들어 놓기
for k in ks :
    model = KMeans(n_clusters = k, n_init = 5)  # n_init 숫자 클수록 계산량 증가하니까 grid search 단계에서는 작게 설정
    %time model.fit(df)    # 'df'라는 이름의 dataset을 사용하여 모델 학습 & 학습에 소요되는 시간 측정  
    inertias.append(model.inertia_)    # 응집도 결과를 inertias 리스트에 계속 저장(추가)
    print('n_cluster : {}, inertia : {}'.format(k, model.inertia_))    # k 설정에 따른 결과 출력
    
# Visualization
plt.figure(figsize = (15, 6))   
plt.plot(ks, inertias, '-o')    
plt.xlabel('number of clusters, k')    
plt.ylabel('inertia')    
plt.xticks(ks)    
plt.show()

```
   
![inertia](https://drive.google.com/uc?id=14f4KoAZfA2qJsnaDp1L1ms9EdSC_FIsf)
   
위의 결과를 보시면 k의 결과가 7개를 넘어가면서부터 k 갯수 증가에 따른 응집도 감소 효과가 점차 작아지는 모습을 볼 수 있습니다. 또한 k=10 을 전후하여 응집도 변화가 다시 일시적으로 커지는 것처럼 보이기 때문에 실제로 분석을 진행할 때에는 k를 7~11로 다양하게 설정하여 클러스터링 작업을 진행하고 군집화 결과를 검토하는데 사용했습니다. 

<br>



### 실루엣(silhouette) 계수

**실루엣 계수**는 inertiua value와 마찬가지로 **군집화의 정답(타겟값)을 모를 때 군집 응집도를 평가**하는 방법 중 한 가지입니다.     

우선 모든 데이터 쌍 (i, j)에 대해 거리 혹은 비유사도(dissimilarity)를 구하고, 이 결과를 이용하여 모든 데이터 i에 대해 다음 값을 구합니다.     

- a(i) : i와 같은 군집에 속한 원소들의 평균 거리
- b(i) : i와 다른 군집 중 가장 가까운 군집까지의 평균 거리
- 이 때 데이터 i에 대한 실루엣 계수는 아래와 같이 정의합니다. 전체 데이터의 실루엣 계수를 평균된 값을 평균 실루엣 계수라고 합니다.  
    
$$ s(i)=\frac { b(i)-a(i) }{ \max { \{ a(i),b(i)\}  }  } $$
    
만약 데이터 i 에 대하여 같은 군집의 데이터가 다른 군집의 데이터보다 더 가깝다면 그 데이터의 실수엣 계수는 양수(최대 점수 1)가 됩니다. 반대로 다른 군집의 데이터가 같은 군집의 데이터보다 더 가깝다면 군집화가 잘못된 경우라고 볼 수 있는데요, 이 때는 그 데이터의 실루엣 계수가 음수가 됩니다(-1: 잘못된 군집). 잘못된 군집화에서는 실루엣계수가 음수인 데이터가 많아지므로 평균 실루엣 계수가 작아집니다. 따라서 **실루엣 계수가 클수록 좋은 군집화**라고 할 수 있습니다.    

하지만, 결론부터 말씀드리면 최근에 클러스터링 작업을 진행할 때는 실루엣 계수를 활용하지는 않았는데요, 그 이유는..    

- 클러스터 모양이 밀집되었을 때(원형일 때)는 성능이 좋지만, **모양이 복잡할 때는 밀집도를 활용한 평가가 잘 들어맞지 않는 문제**가 있고, *(사실 이건 inertia value를 구할 때도 같은 문제가 발생하긴 합니다 )*    
- inertia value를 구할 때보다 **계산량이 많아서** 대용량 데이터셋으로 k 갯수별로 계수를 구하는 것에 시간이 너무 오래 걸리는 이슈도 있었습니다.     
- 또한, 밀집도 확인을 위해 굳이 2가지 군집타당성 지표를 활용하는 것은 그리 효율적이지 못하다고 판단했기 때문입니다. 실루엣 점수가 높은 k를 찾더라도 **군집에 어떤 유의미한 발견이나 흥미로운 내용이 있는지 여전히 알 수 없는 문제**가 있기 때문에 응집도 지표는 inertia value로 충분할 것으로 판단했습니다.    

아래는 실루엣 계수를 구하기 위한 모듈과 간단한 명령어 예시입니다. 참고로 보시면 될 것 같습니다.    

```python
from sklearn.metrics.cluster import silhouette_score    # 모듈 불러오기

model = KMeans(n_clusters = k)  # 모델 생성. k에는 적정한 군집수 넣기
k_model = model.fit(df)    # 'df'라는 이름의 dataset으로 모델 학습 
cluster = k_model.labels_    # 클러스터링 결과를 cluster라는 변수에 저장

print("Silhouette Score:", silhouette_score(df, cluster))    # dataset과 결과변수를 함수에 넣어 계수 출력
```
좀더 자세한 내용과 활용 방법은 아래 링크를 참고하시면 좋을 것 같습니다.    
※ 참고 : [Selecting the number of clusters with silhouette analysis on KMeans clustering]('https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py')



<br>

### 군집 응집도, 비지도 학습의 한계

앞서 k-means 알고리즘을 사용할 때 활용가능한 2가지 군집 응집도 계산 방법을 살펴보았는데요, 그 어느 것도 적정 군집수를 마법처럼 콕 집어 알려주지는 않습니다. 다만, **아주 잘못된 결과를 처음부터 만드는 우를 범하지 않도록 시행착오를 줄여주는 데에 유용**하게 사용할 수 있을 것으로 보여집니다. 또한, 앞서 말씀드렸던 것처럼 비지도 학습은 정답이 없기 때문에 군집화 결과물을 평가하기 위해서는 **결국 사람이 직접 확인하는 과정**을 거칠 수밖에 없기도 합니다.     

실제로 저희 팀에서 최근에 진행한 고객세분화 프로젝트에서는 데이터 준비 작업과 군집화보다 **1)군집화 결과물을 이해하기 쉽게 시각화**하고, **2)현업(마케터, PO 등) 담당자들과 군집화가 잘 되었는지 검토**하는 과정에 훨씬 많은 시간을 쏟았던 것으로 기억합니다. 물론 이 과정에서 **군집의 수와 데이터 추출 조건을 달리하여 수차례 군집화 작업을 다시 진행**한 것은 당연한 일이겠지요. 프로파일링(군집별로 특성을 요약&시각화) 작업만 10번 가까이 반복했던 것으로 기억합니다. 그 과정에서 일부 클러스터(군집)는 활용 목적에 맞게 임의로 통합하는 작업을 병행하기도 했습니다. 

결국 클러스터링 분석 작업에서 납득할 만한, 그리고 현업에서 활용 가능한 결과물을 만들어내기 위해서는 온전히 기계에만 의존하는 것은 한계가 있습니다. 이것은 비단 k-means를 활용하는 작업뿐만 아니라 다른 알고리즘을 사용한 군집화 작업에서도 마찬가지입니다. 이렇게 보면 클러스터링 분석 결과물의 퀄리티는 '공들인 시간'과 상관관계가 높을 수도 있을 것 같네요.    

오늘 이야기는 여기서 마치겠습니다. 긴 글 읽어주셔서 감사합니다 :)    

<br>


---
*클러스터링 작업뿐만 아니라 여러 학습 과정에서 아래 블로그의 도움을 많이 받았습니다. 감사합니다.*
*매우 유명한 블로그이고 제가 다루는 것보다 훨씬 깊은 내용을 자세히 배울 수 있습니다.*

[조대협의 블로그]('https://bcho.tistory.com/1203')
[ratsgo's blog]('https://ratsgo.github.io/machine%20learning/2017/04/16/clustering')
[TOTAL DATA SCIENCE]('https://euriion.com')

<br>


























### 데이터 스케일이란?

데이터 **스케일(scale)**은 데이터의 **크기, 규모, 범위**를 의미합니다. 

|이름|간식비용(원)|식사비용(원)|
|:-:|:-:|:-:|
|종원|10,000|75,000|
|여운|0|25,000|
|상인|6,000|60,000|
|지윤|8,000|55,000|

예시를 한 번 보실까요? 4명의 친구가 지난 일주일간 간식 구입과 점심 식대로 지불한 비용을 표로 정리했습니다. 간식 비용은 0~10,000원에서 값이 분포되어 있고, 식사비용은 25,000~75,000원 사이에 값이 분포되어 있습니다.  

변수끼리 스케일 차이가 크게 벌어지게 되면, 값을 비교할 때 판단을 쉽게 할 수 없는 문제가 발생합니다. 예를 들어, 식사 비용보다 간식 비용에 상대적으로 돈을 더 많이 지불한 사람은 누구일까요? 머릿속으로 쉽게 계산하기 어렵습니다. 만약 비교해야 할 사람도 많고 변수의 종류도 많다면 비교가 더욱 어려워지겠죠?

데이터 스케일링은 **개체나 변수간의 비교 혹은 차이 계산이 필요한 상황**에서 **스케일 조정**을 통해 그런 작업들을 보다 수월하게 해주는 것을 의미합니다. 다음과 같이 스케일 조정을 거친 변수를 하나 더 추가해 볼께요.

|이름|간식비용(원)|식사비용(원)|식사비용(원)_scaled|
|:-:|:-:|:-:|:-:|
|종원|10,000|75,000|10,000|
|여운|0|25,000|0|
|상인|6,000|60,000|7,000|
|지윤|8,000|55,000|6,000|

스케일을 조정한 변수는 가장 많은 식사 비용이었던 75,000을 가장 높은 간식 비용인 10,000원에 맞추고, 가장 적은 식사 비용인 25,000을 가장 적은 간식 비용이었던 0에 맞추어서 비율에 맞게 다시 계산한 값으로 채웠습니다. 이번에는 아까 했던 질문에 좀더 쉽게 답을 할 수 있을 것 같습니다. 누가 식사 비용보다 간식 비용에 좀더 돈을 지불한 셈일까요? *(정답은 지윤 ! )*




<br>
### 클러스터링 분석을 위한 필수 작업

이전에 범주형 변수와 결측값 이야기를 다루면서 **거리 기반 클러스터링 분석에서는 개체끼리 특성 차이가 얼마나 나는지 계산이 필요하다**는 이야기를 했던 것 기억하시나요? 클러스터링 분석에서는 단일 변수(특성)에 대한 차이 계산뿐만 아니라, 둘 이상의 변수들을 모두 고려하여 개체간의 차이 계산이 필요합니다. (왜냐하면 어떤 개체끼리 비슷한지 판단하기 위해 보통 여러가지의 특성들을 고려하기 때문입니다.) 그렇기 때문에 **변수끼리 스케일 차이가 크게 벌어지면 스케일이 큰 변수의 영향력이 상대적으로 커지기 때문에 분석 결과가 잘못될 가능성이 높아지게 됩니다.**

**스케일 조정이 클러스터링 분석에 앞서 반드시 필요한 이유**도 바로 여기에 있습니다. 거리 기반 알고리즘을 사용하는 클러스터링 분석 뿐만 아니라 신경망 모형, SVM 같은 알고리즘도 데이터 스케일에 민감하기 때문에 데이터의 특성값을 조정하는 방법에 대해 미리 알아두는 것도 필요합니다. 


<br>
### 여러가지 스케일 조정 방법

보통은 각각의 변수별로 전처리 작업을 진행한 후 분석 혹은 모델링 작업에 사용할 변수 모두를 일괄적으로 스케일 조정하는 과정을 거치게 됩니다. 아래는 대표적인 스케일링 방법들이고, 프로그램 언어 라이브러리마다 쉽게 활용할 수 있는 모듈을 제공하고 있습니다.

#### 1. Standard Scaler
```
z = (X - u) / s
※ u : 변수 X의 평균, s : 변수 X의 표준편차
```
 [[참고] sklearn.preprocessing.StandardScaler]('https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler')
- 각 변수(특성)의 평균을 0, 분산을 1로 변경하여 모든 변수가 같은 크기를 갖게 함.
- 변수의 최솟값과 최댓값 크기를 제한하지 않음. 
- 따라서, 이상값(outlier)이 있는 경우 평균과 표준편차에 영향을 미치기 때문에 데이터 분포(확산)에 영향을 미침.

#### 2. MinMax Scaler
```
X_std = (X - X.min) / (X.max - X.min)
X_scaled = X_std * (max - min) + min
```
[[참고] sklearn.preprocessing.MinMaxScaler]('https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler')
- 모든 특성이 정확하게 0과 1 사이에 위치하도록 데이터를 변형
- Standard scaler와 마찬가지로 이상값의 영향에 민감함.

#### 3. Robust Scaler
```
X_scaled = (X - q2) / (q3 - q1)
※ q2 : 중앙값, q1 : 1사분위값, q3 : 3사분위값
```
[[참고] sklearn.preprocessing.RobustScaler]('https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler')
- 위의 2가지 방법과는 다르게 이상값의 영향을 최소화
- 중앙값(medain)과 사분위값(quartile)을 사용

위의 3가지 이외에도 [Normalizer]('https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html#sklearn.preprocessing.normalize'), [QuantileTransformer]('https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html#sklearn.preprocessing.QuantileTransformer') 등의 스케일링 방법도 있습니다. 어떤 스케일 조정 방법을 사용할지는 데이터의 특징, 분포 등을 검토한 후에 결정하면 됩니다. 

저의 경우에는 최근 회사에서 클러스터링 분석을 진행할 때 변수별로 이상값(그 중에서도 특히 과대값) 전처리를 진행한 후 MinMaxScaler를 적용했습니다. 사실 RobustScaler와 MinMaxScaler를 모두 적용하여 클러스터링 결과를 비교해보았는데요, MinMaxScaler 방식을 적용한 결과가 클러스터간 특징 구분이 좀더 수월한 면이 있었기 때문에 최종적으로는 MinMaxScaler를 사용했습니다. 꼭 한 가지 방법만을 고집할 필요는 없을 것 같고요, 클러스터링 분석 자체가 워낙 여러가지 다양한 시도를 반복해야하는 작업인지라 이런저런 방법을 시도해 보시고 좀더 적절한 방법을 선택하시면 될 것 같습니다. 


<br>
### 다음 시간에는..

3번에 걸쳐 클러스터링 분석용 데이터 준비 작업에 대한 이야기를 했는데요. 다음에는 **데이터 전처리를 할 때 자주 사용하는 Python 코드**에 대해서 한 번 정리해볼까 합니다. 오늘도 긴 글 읽어주셔서 감사합니다!!














